{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>I'm wired I know I'm George I was made that wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>What amazing service! Apple won't even talk to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0  #fingerprint #Pregnancy Test https://goo.gl/h1...\n",
       "1   2      0  Finally a transparant silicon case ^^ Thanks t...\n",
       "2   3      0  We love this! Would you go? #talk #makememorie...\n",
       "3   4      0  I'm wired I know I'm George I was made that wa...\n",
       "4   5      1  What amazing service! Apple won't even talk to..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../dataset/sentiment_analysis.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       0\n",
       "label    0\n",
       "tweet    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert upper case to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #fingerprint #pregnancy test https://goo.gl/h1...\n",
       "1    finally a transparant silicon case ^^ thanks t...\n",
       "2    we love this! would you go? #talk #makememorie...\n",
       "3    i'm wired i know i'm george i was made that wa...\n",
       "4    what amazing service! apple won't even talk to...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"] = data[\"tweet\"].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "data[\"tweet\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #fingerprint #pregnancy test  #android #apps #...\n",
       "1    finally a transparant silicon case ^^ thanks t...\n",
       "2    we love this! would you go? #talk #makememorie...\n",
       "3    i'm wired i know i'm george i was made that wa...\n",
       "4    what amazing service! apple won't even talk to...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"] = data['tweet'].apply(lambda x: \" \".join(re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE) for x in x.split()))\n",
    "data[\"tweet\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    fingerprint pregnancy test  android apps beaut...\n",
       "1    finally a transparant silicon case  thanks to ...\n",
       "2    we love this would you go talk makememories un...\n",
       "3    im wired i know im george i was made that way ...\n",
       "4    what amazing service apple wont even talk to m...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "data[\"tweet\"] = data[\"tweet\"].apply(remove_punctuations)\n",
    "data[\"tweet\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_29532\\4038800621.py:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  data[\"tweet\"] = data['tweet'].str.replace('\\d+', '', regex=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>fingerprint pregnancy test  android apps beaut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>finally a transparant silicon case  thanks to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>we love this would you go talk makememories un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>im wired i know im george i was made that way ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>what amazing service apple wont even talk to m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0  fingerprint pregnancy test  android apps beaut...\n",
       "1   2      0  finally a transparant silicon case  thanks to ...\n",
       "2   3      0  we love this would you go talk makememories un...\n",
       "3   4      0  im wired i know im george i was made that way ...\n",
       "4   5      1  what amazing service apple wont even talk to m..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"] = data['tweet'].str.replace('\\d+', '', regex=True)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to ../Static/model...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords', download_dir='../Static/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Static/model/corpora/stopwords/english', 'r') as file:\n",
    "    sw = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tweet\"] = data[\"tweet\"].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "data[\"tweet\"] = data[\"tweet\"].apply(lambda x: \" \".join(ps.stem(x) for x in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vacabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "vocab = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in data['tweet']:\n",
    "    vocab.update(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test',\n",
       " 'android',\n",
       " 'app',\n",
       " 'beauti',\n",
       " 'cute',\n",
       " 'health',\n",
       " 'iger',\n",
       " 'iphoneonli',\n",
       " 'iphonesia',\n",
       " 'iphon',\n",
       " 'final',\n",
       " 'case',\n",
       " 'thank',\n",
       " 'yay',\n",
       " 'soni',\n",
       " 'xperia',\n",
       " 'love',\n",
       " 'would',\n",
       " 'go',\n",
       " 'talk',\n",
       " 'relax',\n",
       " 'smartphon',\n",
       " 'wifi',\n",
       " 'connect',\n",
       " 'im',\n",
       " 'know',\n",
       " 'made',\n",
       " 'way',\n",
       " 'home',\n",
       " 'amaz',\n",
       " 'servic',\n",
       " 'appl',\n",
       " 'wont',\n",
       " 'even',\n",
       " 'question',\n",
       " 'pay',\n",
       " 'stupid',\n",
       " 'support',\n",
       " 'softwar',\n",
       " 'updat',\n",
       " 'fuck',\n",
       " 'phone',\n",
       " 'big',\n",
       " 'time',\n",
       " 'happi',\n",
       " 'us',\n",
       " 'instap',\n",
       " 'instadaili',\n",
       " 'xperiaz',\n",
       " 'new',\n",
       " 'type',\n",
       " 'c',\n",
       " 'charger',\n",
       " 'cabl',\n",
       " 'uk',\n",
       " '…',\n",
       " 'amazon',\n",
       " 'year',\n",
       " 'newyear',\n",
       " 'start',\n",
       " 'technolog',\n",
       " 'samsunggalaxi',\n",
       " 'iphonex',\n",
       " 'shop',\n",
       " 'listen',\n",
       " 'music',\n",
       " 'likeforlik',\n",
       " 'photo',\n",
       " 'fun',\n",
       " 'selfi',\n",
       " 'water',\n",
       " 'camera',\n",
       " 'picoftheday',\n",
       " 'sun',\n",
       " 'instagood',\n",
       " 'boy',\n",
       " 'outdoor',\n",
       " 'hey',\n",
       " 'make',\n",
       " 'ipod',\n",
       " 'dont',\n",
       " 'color',\n",
       " 'inch',\n",
       " 'crash',\n",
       " 'everi',\n",
       " 'need',\n",
       " 'realli',\n",
       " 'drop',\n",
       " 'ball',\n",
       " 'design',\n",
       " 'give',\n",
       " 'anoth',\n",
       " 'crazi',\n",
       " 'purchas',\n",
       " 'lol',\n",
       " 'work',\n",
       " 'hard',\n",
       " 'play',\n",
       " 'ipad',\n",
       " 'batteri',\n",
       " 'charg',\n",
       " 'dead',\n",
       " 'saturday',\n",
       " 'summer',\n",
       " 'like',\n",
       " 'share',\n",
       " 'want',\n",
       " 'instagram',\n",
       " 'photooftheday',\n",
       " 'tweegram',\n",
       " 'reason',\n",
       " 'one',\n",
       " 'suck',\n",
       " 'truth',\n",
       " 'agre',\n",
       " 'fact',\n",
       " 'store',\n",
       " 'screen',\n",
       " 'monday',\n",
       " 'ur',\n",
       " 'art',\n",
       " 'easter',\n",
       " 'dear',\n",
       " 'friend',\n",
       " 'face',\n",
       " 'email',\n",
       " 'seem',\n",
       " 'pie',\n",
       " 'wife',\n",
       " 'ive',\n",
       " 'day',\n",
       " 'button',\n",
       " 'back',\n",
       " 'broke',\n",
       " 'hit',\n",
       " 'goe',\n",
       " 'complet',\n",
       " 'black',\n",
       " 'keep',\n",
       " 'get',\n",
       " 'text',\n",
       " 'cant',\n",
       " 'check',\n",
       " 'wallpap',\n",
       " 'wall',\n",
       " 'galaxi',\n",
       " 'samsung',\n",
       " 'patent',\n",
       " 'million',\n",
       " 'parti',\n",
       " 'mess',\n",
       " 'havent',\n",
       " 'done',\n",
       " 'noth',\n",
       " 'touch',\n",
       " 'lose',\n",
       " 'pic',\n",
       " 'kill',\n",
       " 'someon',\n",
       " 'hateappl',\n",
       " 'flower',\n",
       " 'green',\n",
       " 'must',\n",
       " 'watch',\n",
       " 'youtub',\n",
       " 'subscrib',\n",
       " 'daili',\n",
       " 'vlog',\n",
       " 'twitch',\n",
       " 'game',\n",
       " 'ps',\n",
       " 'xbox',\n",
       " 'io',\n",
       " 'live',\n",
       " 'laugh',\n",
       " 'life',\n",
       " 'food',\n",
       " 'instago',\n",
       " 'instahub',\n",
       " 'instagram…',\n",
       " 'friendship',\n",
       " 'dog',\n",
       " 'famili',\n",
       " 'goal',\n",
       " 'bestfriend',\n",
       " 'america',\n",
       " 'taken',\n",
       " 'sunset',\n",
       " 'sky',\n",
       " 'sister',\n",
       " 'bought',\n",
       " 'earli',\n",
       " 'bday',\n",
       " 'gift',\n",
       " 'receiv',\n",
       " 'note',\n",
       " 'mani',\n",
       " 'market',\n",
       " 'delet',\n",
       " 'song',\n",
       " 'itun',\n",
       " 'freak',\n",
       " 'window',\n",
       " 'advanc',\n",
       " 'custom',\n",
       " 'stand',\n",
       " 'bad',\n",
       " 'cheap',\n",
       " 'tech',\n",
       " 'bull',\n",
       " 'smile',\n",
       " 'creat',\n",
       " 'let',\n",
       " 'sunday',\n",
       " 'alway',\n",
       " 'eye',\n",
       " 'ootd',\n",
       " 'fashion',\n",
       " 'blackandwhit',\n",
       " 'film',\n",
       " 'set',\n",
       " 'video',\n",
       " 'produc',\n",
       " 'follow',\n",
       " 'movi',\n",
       " 'act',\n",
       " 'pink',\n",
       " 'sweet',\n",
       " 'sexi',\n",
       " 'ladi',\n",
       " 'week',\n",
       " 'end',\n",
       " 'iphoneplu',\n",
       " 'moment',\n",
       " 'see',\n",
       " 'differ',\n",
       " 'photographi',\n",
       " 'natur',\n",
       " 'landscap',\n",
       " 'view',\n",
       " 'tree',\n",
       " 'travel',\n",
       " 'googl',\n",
       " 'cut',\n",
       " 'program',\n",
       " 'look',\n",
       " 'got',\n",
       " 'christma',\n",
       " 'girl',\n",
       " 'instacool',\n",
       " 'free',\n",
       " 'appstor',\n",
       " 'joy',\n",
       " 'peac',\n",
       " 'reflect',\n",
       " 'rememb',\n",
       " 'cloud',\n",
       " 'gr',\n",
       " 'iphone…',\n",
       " 'babi',\n",
       " 'pet',\n",
       " 'news',\n",
       " 'fail',\n",
       " 'funni',\n",
       " 'hate',\n",
       " 'tablet',\n",
       " 'person',\n",
       " 'use',\n",
       " 'fan',\n",
       " 'think',\n",
       " 'product',\n",
       " 'friday',\n",
       " 'call',\n",
       " 'blackfriday',\n",
       " 'holiday',\n",
       " 'newyork',\n",
       " 'busi',\n",
       " 'money',\n",
       " 'birthday',\n",
       " 'tv',\n",
       " 'comput',\n",
       " 'school',\n",
       " 'serious',\n",
       " 'month',\n",
       " 'good',\n",
       " 'job',\n",
       " 'actual',\n",
       " '£',\n",
       " 'replac',\n",
       " 'that',\n",
       " 'still',\n",
       " 'rt',\n",
       " 'droid',\n",
       " 'cool',\n",
       " 'pictur',\n",
       " 'l',\n",
       " 'run',\n",
       " 'beach',\n",
       " 'sport',\n",
       " 'bit',\n",
       " 'hashtag',\n",
       " 'yet',\n",
       " 'arriv',\n",
       " 'gain',\n",
       " 'everyon',\n",
       " 'sougofollow',\n",
       " 'ff',\n",
       " 'iphoneographi',\n",
       " 'iphonephotographi',\n",
       " 'mobil',\n",
       " 'bright',\n",
       " 'user',\n",
       " 'date',\n",
       " 'less',\n",
       " 'random',\n",
       " 'instamood',\n",
       " 'wine',\n",
       " 'creativ',\n",
       " 'hot',\n",
       " 'icon',\n",
       " 'origin',\n",
       " 'pop',\n",
       " 'red',\n",
       " 'rock',\n",
       " 'soul',\n",
       " 'singer',\n",
       " 'univers',\n",
       " 'wed',\n",
       " 'thought',\n",
       " 'id',\n",
       " 'lost',\n",
       " 'ipadmini',\n",
       " 'feel',\n",
       " 'broken',\n",
       " 'light',\n",
       " 'pleas',\n",
       " 'indonesia',\n",
       " 'gold',\n",
       " 'potd',\n",
       " 'reset',\n",
       " 'sorri',\n",
       " 'white',\n",
       " 'tea',\n",
       " 'chill',\n",
       " 'cover',\n",
       " 'g',\n",
       " 'came',\n",
       " 'magic',\n",
       " 'come',\n",
       " 'followsunday',\n",
       " 'followback',\n",
       " 'teamfollowback',\n",
       " 'retweet',\n",
       " 'ya',\n",
       " 'thing',\n",
       " 'i’m',\n",
       " 'it’',\n",
       " 'alreadi',\n",
       " 'problem',\n",
       " 'issu',\n",
       " 'abl',\n",
       " 'sonya',\n",
       " 'shoot',\n",
       " 'put',\n",
       " 'price',\n",
       " 'devic',\n",
       " 'win',\n",
       " 'box',\n",
       " 'memori',\n",
       " 'brother',\n",
       " '–',\n",
       " 'oh',\n",
       " 'lip',\n",
       " 'enjoy',\n",
       " 'playstat',\n",
       " 'gamer',\n",
       " 'someth',\n",
       " 'wrong',\n",
       " 'right',\n",
       " 'today',\n",
       " 'earphon',\n",
       " 'lifestyl',\n",
       " 'fuckyou',\n",
       " 'never',\n",
       " 'bug',\n",
       " 'littl',\n",
       " 'qualiti',\n",
       " 'girlfriend',\n",
       " 'card',\n",
       " 'z',\n",
       " 'present',\n",
       " 'mom',\n",
       " 'macbookpro',\n",
       " 'macbook',\n",
       " 'quot',\n",
       " 'word',\n",
       " 'tweetgram',\n",
       " 'great',\n",
       " 'repair',\n",
       " 'hour',\n",
       " 'everyth',\n",
       " 'mode',\n",
       " 'usa',\n",
       " 'compani',\n",
       " 'model',\n",
       " 'cd',\n",
       " 'featur',\n",
       " 'didnt',\n",
       " 'coffe',\n",
       " 'effect',\n",
       " 'spring',\n",
       " 'galaxynot',\n",
       " 'special',\n",
       " 'valentin',\n",
       " 'nowplay',\n",
       " 'daughter',\n",
       " 'poem',\n",
       " 'car',\n",
       " 'sign',\n",
       " 'lunch',\n",
       " 'park',\n",
       " 'banana',\n",
       " 'autumn',\n",
       " 'spend',\n",
       " 'much',\n",
       " 'book',\n",
       " 'say',\n",
       " 'u',\n",
       " 'took',\n",
       " 'download',\n",
       " 'ad',\n",
       " 'twitter',\n",
       " 'educ',\n",
       " 'n',\n",
       " 'miss',\n",
       " 'last',\n",
       " 'min',\n",
       " 'tmobil',\n",
       " 'rid',\n",
       " 'absolut',\n",
       " 'annoy',\n",
       " 'level',\n",
       " 'buy',\n",
       " 'full',\n",
       " 'version',\n",
       " 'import',\n",
       " 'mood',\n",
       " 'blog',\n",
       " 'style',\n",
       " 'bestoftheday',\n",
       " 'pretti',\n",
       " 'babe',\n",
       " 'send',\n",
       " 'turn',\n",
       " 'imessag',\n",
       " 'sleep',\n",
       " 'popular',\n",
       " 'tweet',\n",
       " 'shotoniphon',\n",
       " 'photograph',\n",
       " 'sync',\n",
       " 'second',\n",
       " 'fml',\n",
       " 'candi',\n",
       " 'nice',\n",
       " 'wait',\n",
       " 'hand',\n",
       " 'gb',\n",
       " 'first',\n",
       " 'bar',\n",
       " 'key',\n",
       " 'long',\n",
       " 'cold',\n",
       " 'boot',\n",
       " 'siri',\n",
       " 'doesnt',\n",
       " 'liter',\n",
       " 'cri',\n",
       " 'contact',\n",
       " 'wonder',\n",
       " 'avail',\n",
       " 'b',\n",
       " 'laptop',\n",
       " 'vaio',\n",
       " 'blackberri',\n",
       " 'best',\n",
       " 'fruit',\n",
       " 'fall',\n",
       " 'soon',\n",
       " 'yum',\n",
       " 'mac',\n",
       " 'display',\n",
       " 'told',\n",
       " 'stop',\n",
       " 'p',\n",
       " 'three',\n",
       " 'ship',\n",
       " 'gear',\n",
       " 'well',\n",
       " 'past',\n",
       " 'singl',\n",
       " 'capetownsup',\n",
       " 'sup',\n",
       " 'surf',\n",
       " 'capetown',\n",
       " 'pro',\n",
       " 'half',\n",
       " 'stuff',\n",
       " 'excit',\n",
       " 'open',\n",
       " 'mine',\n",
       " 'piss',\n",
       " 'offici',\n",
       " 'keyboard',\n",
       " 'okay',\n",
       " 'though',\n",
       " 'enough',\n",
       " 'simpl',\n",
       " 'th',\n",
       " 'refus',\n",
       " 'night',\n",
       " 'son',\n",
       " 'instagood…',\n",
       " 'jj',\n",
       " 'makeup',\n",
       " 'valentinesday',\n",
       " 'februari',\n",
       " 'portrait',\n",
       " 'shot',\n",
       " 'sonyalpha',\n",
       " 'mm',\n",
       " 'password',\n",
       " 'zoom',\n",
       " 'stevejob',\n",
       " 'yall',\n",
       " 'addict',\n",
       " 'prophet',\n",
       " 'husband',\n",
       " 'kindl',\n",
       " 'a…',\n",
       " 'upgrad',\n",
       " 'help',\n",
       " 'children',\n",
       " 'there',\n",
       " 'least',\n",
       " 'tab',\n",
       " 'real',\n",
       " 'visit',\n",
       " 'hi',\n",
       " 'world',\n",
       " 'old',\n",
       " 'followm',\n",
       " 'likelik',\n",
       " 'samsung…',\n",
       " 'swag',\n",
       " 'cat',\n",
       " 'edit',\n",
       " 'sick',\n",
       " 'paint',\n",
       " 'bullshit',\n",
       " 'may',\n",
       " 'perfect',\n",
       " 'instaphoto',\n",
       " 'welcom',\n",
       " 'draw',\n",
       " 'os',\n",
       " 'throw',\n",
       " 'fast',\n",
       " 'w',\n",
       " 'take',\n",
       " 'two',\n",
       " 'next',\n",
       " 'offer',\n",
       " 'middl',\n",
       " 'access',\n",
       " 'account',\n",
       " 'find',\n",
       " 'citi',\n",
       " 'stori',\n",
       " 'destini',\n",
       " 'awesom',\n",
       " 'accessori',\n",
       " 'info',\n",
       " 'goodnight',\n",
       " 'dream',\n",
       " 'hope',\n",
       " 'uae',\n",
       " 'lucki',\n",
       " 'deal',\n",
       " 'passion',\n",
       " 'read',\n",
       " 'edm',\n",
       " 'whole',\n",
       " 'playlist',\n",
       " 'god',\n",
       " 'nx',\n",
       " 'cuti',\n",
       " 'high',\n",
       " 'usb',\n",
       " 'geek',\n",
       " 'bot',\n",
       " 'gadget',\n",
       " 'power',\n",
       " 'pc',\n",
       " 'sprint',\n",
       " 'pick',\n",
       " 'wish',\n",
       " 'minut',\n",
       " 'count',\n",
       " 'tl',\n",
       " 'drive',\n",
       " 'nyc',\n",
       " 'gay',\n",
       " 'readi',\n",
       " 'cellphon',\n",
       " 'space',\n",
       " 'ny',\n",
       " 'tattoo',\n",
       " 'total',\n",
       " 'ye',\n",
       " 'via',\n",
       " 'air',\n",
       " 'instal',\n",
       " 'fit',\n",
       " 'plu',\n",
       " 'sim',\n",
       " 'florida',\n",
       " 'sale',\n",
       " 'nokia',\n",
       " 'motorola',\n",
       " 'lg',\n",
       " 'without',\n",
       " 'hold',\n",
       " 'speed',\n",
       " 'unitedst',\n",
       " 'guitarplay',\n",
       " 'smart',\n",
       " 'crap',\n",
       " 'calendar',\n",
       " 'event',\n",
       " 'icloud',\n",
       " 'angri',\n",
       " 'bird',\n",
       " 'freez',\n",
       " 'ever',\n",
       " 'sinc',\n",
       " 'team',\n",
       " 'tri',\n",
       " 'convers',\n",
       " 'wouldnt',\n",
       " 'small',\n",
       " 'blue',\n",
       " '—',\n",
       " 'steemit',\n",
       " 'sonylen',\n",
       " 'len',\n",
       " 'sonyphotographi',\n",
       " 'imag',\n",
       " 'photofe',\n",
       " 'feed',\n",
       " 'yeah',\n",
       " 'gorgeou',\n",
       " 'ig',\n",
       " 'orang',\n",
       " 'haha',\n",
       " 'dress',\n",
       " 'lock',\n",
       " 'speaker',\n",
       " 'reallyr',\n",
       " 'colleg',\n",
       " 'true',\n",
       " 'chocol',\n",
       " 'shit',\n",
       " 'ador',\n",
       " 'nofilt',\n",
       " 'drink',\n",
       " 'purpl',\n",
       " 'tasti',\n",
       " 'garden',\n",
       " 'андроид',\n",
       " 'guy',\n",
       " 'bestpric',\n",
       " 'jun',\n",
       " 'unlock',\n",
       " 'caus',\n",
       " 'manag',\n",
       " 'mommi',\n",
       " 'bless',\n",
       " 'could',\n",
       " 'chines',\n",
       " 'close',\n",
       " 'system',\n",
       " 'morn',\n",
       " 'nike',\n",
       " 'goodmorn',\n",
       " 'r',\n",
       " 'imac',\n",
       " 'sell',\n",
       " 'mad',\n",
       " 'purpos',\n",
       " 'pari',\n",
       " 'answer',\n",
       " 'roll',\n",
       " 'verizon',\n",
       " 'headphon',\n",
       " 'show',\n",
       " 'sound',\n",
       " 'itali',\n",
       " 'socialmedia',\n",
       " 'learn',\n",
       " 'smoke',\n",
       " 'tomorrow',\n",
       " 'here',\n",
       " 'delici',\n",
       " 'far',\n",
       " 'away',\n",
       " 'ebay',\n",
       " 'fix',\n",
       " 'easi',\n",
       " 'silver',\n",
       " 'oneplu',\n",
       " 'provid',\n",
       " 'experi',\n",
       " 'flag',\n",
       " 'emoji',\n",
       " 'chang',\n",
       " 'al',\n",
       " 'april',\n",
       " 'man',\n",
       " 'wanna',\n",
       " 'your',\n",
       " 'tell',\n",
       " 'kid',\n",
       " 'backup',\n",
       " 'peopl',\n",
       " 'dad',\n",
       " 'anyon',\n",
       " 'els',\n",
       " 'block',\n",
       " 'number',\n",
       " 'possibl',\n",
       " 'frustrat',\n",
       " 'file',\n",
       " 'bro',\n",
       " 'rhyme',\n",
       " 'thx',\n",
       " 'jailbreak',\n",
       " 'sit',\n",
       " 'restor',\n",
       " 'stock',\n",
       " 'mother',\n",
       " 'igdaili',\n",
       " 'marri',\n",
       " 'healthi',\n",
       " 'ugh',\n",
       " 'x',\n",
       " 'bc',\n",
       " 'gratitud',\n",
       " 'edg',\n",
       " 'unbox',\n",
       " 'wow',\n",
       " 'sonyphoto',\n",
       " 'top',\n",
       " 'phonecas',\n",
       " 'june',\n",
       " 'sad',\n",
       " 'yellow',\n",
       " 'woman',\n",
       " 'might',\n",
       " 'india',\n",
       " 'canada',\n",
       " 'europ',\n",
       " 'super',\n",
       " 'care',\n",
       " 'map',\n",
       " 'applestor',\n",
       " 'hello',\n",
       " 'order',\n",
       " 'sent',\n",
       " 'went',\n",
       " 'wasnt',\n",
       " 'insta',\n",
       " 'newphon',\n",
       " 'fish',\n",
       " 'london',\n",
       " 'vsco',\n",
       " 'track',\n",
       " 'catch',\n",
       " 'finger',\n",
       " 'anymor',\n",
       " 'captur',\n",
       " 'futur',\n",
       " 'cell',\n",
       " 'instalik',\n",
       " 'sony…',\n",
       " 'march',\n",
       " 'proud',\n",
       " 'hateiphon',\n",
       " 'sticker',\n",
       " 'vscocam',\n",
       " 'bring',\n",
       " 'better',\n",
       " 'kiss',\n",
       " 'heart',\n",
       " 'fresh',\n",
       " 'librari',\n",
       " 'inspir',\n",
       " 'inlov',\n",
       " 'luxuri',\n",
       " 'exquisit',\n",
       " 'squishi',\n",
       " 'charm',\n",
       " 'strap',\n",
       " 'toy',\n",
       " 'decor',\n",
       " 'cake',\n",
       " 'theyr',\n",
       " 'lte',\n",
       " 'facebook',\n",
       " 'wipe',\n",
       " 'human',\n",
       " 'bff',\n",
       " 'instalov',\n",
       " 'storag',\n",
       " 'wake',\n",
       " 'shitti',\n",
       " 'wast',\n",
       " 'switch',\n",
       " 'blond',\n",
       " 'load',\n",
       " 'sue',\n",
       " 'teamandroid',\n",
       " 'da',\n",
       " 'ly',\n",
       " 'lion',\n",
       " 'steve',\n",
       " 'also',\n",
       " 'ill',\n",
       " 'osx',\n",
       " 'photoshoot',\n",
       " 'coupl',\n",
       " 'favorit',\n",
       " 'forc',\n",
       " 'newtoy',\n",
       " 'nascar',\n",
       " 'tire',\n",
       " 'appar',\n",
       " 'ago',\n",
       " 'zeeland',\n",
       " 'appletv',\n",
       " 'ask',\n",
       " 'said',\n",
       " 'meet',\n",
       " 'electron',\n",
       " 'gener',\n",
       " 'link',\n",
       " 'code',\n",
       " 'tshirt',\n",
       " 'tbt',\n",
       " 'beat',\n",
       " 'screw',\n",
       " 'videogam',\n",
       " 'motiv',\n",
       " 'sunni',\n",
       " 'nikon',\n",
       " 'useless',\n",
       " 'boyfriend',\n",
       " 'weekend',\n",
       " 'cook',\n",
       " 'japan',\n",
       " 'starbuck',\n",
       " 'almost',\n",
       " 'cousin',\n",
       " 'tag',\n",
       " 'applesuck',\n",
       " 'bye',\n",
       " 'search',\n",
       " 'dinner',\n",
       " 'discount',\n",
       " 'cheer',\n",
       " 'bluetooth',\n",
       " 'wireless',\n",
       " 'data',\n",
       " 'spent',\n",
       " 'puppi',\n",
       " 'vacat',\n",
       " 'anim',\n",
       " 'notebook',\n",
       " 'redbubbl',\n",
       " 'iphonecas',\n",
       " 'king',\n",
       " 'yo',\n",
       " 'left',\n",
       " 'mean',\n",
       " 'what',\n",
       " 'yesterday',\n",
       " 'mayb',\n",
       " 'trump',\n",
       " 'california',\n",
       " 'repost',\n",
       " 'samsungmobil',\n",
       " 'gone',\n",
       " 'social',\n",
       " 'break',\n",
       " 'trip',\n",
       " 'slow',\n",
       " 'rain',\n",
       " 'wtf',\n",
       " 'anyth',\n",
       " 'cuz',\n",
       " 'hd',\n",
       " 'microsoft',\n",
       " 'applewatch',\n",
       " 'duo',\n",
       " 'dj',\n",
       " 'save',\n",
       " 'mr',\n",
       " 'releas',\n",
       " 'gonna',\n",
       " 'entir',\n",
       " 'part',\n",
       " 'die',\n",
       " 'k',\n",
       " 'tip',\n",
       " 'gotta',\n",
       " 'eat',\n",
       " 'walk',\n",
       " 'side',\n",
       " 'explor',\n",
       " 'hous',\n",
       " 'room',\n",
       " 'rs',\n",
       " 'piano',\n",
       " 'v',\n",
       " 'thankyou',\n",
       " 'happen',\n",
       " 'page',\n",
       " 'surpris',\n",
       " 'deliveri',\n",
       " 'messag',\n",
       " 'vocat',\n",
       " 'thailand',\n",
       " 'khaoko',\n",
       " 'ilc',\n",
       " 'snapspeed…',\n",
       " 'merri',\n",
       " 'htc',\n",
       " 'onlin',\n",
       " 'psn',\n",
       " 'flash',\n",
       " '•',\n",
       " 'treat',\n",
       " 'tagsforlik',\n",
       " 'cost',\n",
       " 'crack',\n",
       " 'collect',\n",
       " 'stuck',\n",
       " 'planet',\n",
       " 'men',\n",
       " 'followfollow',\n",
       " 'blow',\n",
       " 'latest',\n",
       " 'consol',\n",
       " 'blogger',\n",
       " 'protect',\n",
       " 'skin',\n",
       " 'choos',\n",
       " 'uniqu',\n",
       " 'piec',\n",
       " 'lot',\n",
       " 'remov',\n",
       " 'secur',\n",
       " 'princess',\n",
       " 'mind',\n",
       " 'becom',\n",
       " 'miami',\n",
       " 'nail',\n",
       " 'rip',\n",
       " 'figur',\n",
       " 'network',\n",
       " 'bitch',\n",
       " 'tuesday',\n",
       " 'f',\n",
       " 'idea',\n",
       " 'fb',\n",
       " 'omg',\n",
       " 'innov',\n",
       " 'player',\n",
       " 'forev',\n",
       " 'satisfi',\n",
       " 'tonight',\n",
       " 'artist',\n",
       " 'sing',\n",
       " 'click',\n",
       " 'hair',\n",
       " 'ride',\n",
       " 'leav',\n",
       " 'colour',\n",
       " 'togeth',\n",
       " 'asshol',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [key for key in vocab if vocab[key] > 10]\n",
    "len(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocabulary(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w', encoding=\"utf-8\")\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "save_vocabulary(tokens, '../Static/model/vocabulary.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divede dataset for tarin and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"tweet\"]\n",
    "y = data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(ds, vocabulary):\n",
    "    vectorized_lst = []\n",
    "    \n",
    "    for sentence in ds:\n",
    "        sentence_lst = np.zeros(len(vocabulary))\n",
    "        \n",
    "        for i in range(len(vocabulary)):\n",
    "            if vocabulary[i] in sentence.split():\n",
    "                sentence_lst[i] = 1\n",
    "                \n",
    "        vectorized_lst.append(sentence_lst)\n",
    "        \n",
    "    vectorized_lst_new = np.asarray(vectorized_lst, dtype=np.float32)\n",
    "    \n",
    "    return vectorized_lst_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_x_train = vectorizer(X_train, tokens)\n",
    "vectorized_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_x_test = vectorizer(X_test, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle imbalance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9464, 1145) (9464,)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "vectorized_x_train_smote, y_train_smote = smote.fit_resample(vectorized_x_train, y_train)\n",
    "print(vectorized_x_train_smote.shape, y_train_smote.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def training_scores(y_act, y_pred):\n",
    "    acc = round(accuracy_score(y_act, y_pred), 3)\n",
    "    pr = round(precision_score(y_act, y_pred), 3)\n",
    "    rec = round(recall_score(y_act, y_pred), 3)\n",
    "    f1 = round(f1_score(y_act, y_pred), 3)\n",
    "    print(f'Training Scores:\\n\\tAccuracy = {acc}\\n\\tPrecision = {pr}\\n\\tRecall = {rec}\\n\\tF1-Score = {f1}')\n",
    "    \n",
    "def validation_scores(y_act, y_pred):\n",
    "    acc = round(accuracy_score(y_act, y_pred), 3)\n",
    "    pr = round(precision_score(y_act, y_pred), 3)\n",
    "    rec = round(recall_score(y_act, y_pred), 3)\n",
    "    f1 = round(f1_score(y_act, y_pred), 3)\n",
    "    print(f'Testing Scores:\\n\\tAccuracy = {acc}\\n\\tPrecision = {pr}\\n\\tRecall = {rec}\\n\\tF1-Score = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "\tAccuracy = 0.94\n",
      "\tPrecision = 0.916\n",
      "\tRecall = 0.968\n",
      "\tF1-Score = 0.941\n",
      "Testing Scores:\n",
      "\tAccuracy = 0.878\n",
      "\tPrecision = 0.731\n",
      "\tRecall = 0.858\n",
      "\tF1-Score = 0.79\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(vectorized_x_train_smote, y_train_smote)\n",
    "\n",
    "y_train_pred = lr.predict(vectorized_x_train_smote)\n",
    "y_test_pred = lr.predict(vectorized_x_test)\n",
    "training_scores(y_train_smote, y_train_pred)\n",
    "validation_scores(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "\tAccuracy = 0.906\n",
      "\tPrecision = 0.869\n",
      "\tRecall = 0.955\n",
      "\tF1-Score = 0.91\n",
      "Testing Scores:\n",
      "\tAccuracy = 0.875\n",
      "\tPrecision = 0.697\n",
      "\tRecall = 0.938\n",
      "\tF1-Score = 0.8\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(vectorized_x_train_smote, y_train_smote)\n",
    "\n",
    "y_train_pred = mnb.predict(vectorized_x_train_smote)\n",
    "y_test_pred = mnb.predict(vectorized_x_test)\n",
    "training_scores(y_train_smote, y_train_pred)\n",
    "validation_scores(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "\tAccuracy = 1.0\n",
      "\tPrecision = 1.0\n",
      "\tRecall = 0.999\n",
      "\tF1-Score = 1.0\n",
      "Testing Scores:\n",
      "\tAccuracy = 0.831\n",
      "\tPrecision = 0.679\n",
      "\tRecall = 0.692\n",
      "\tF1-Score = 0.685\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(vectorized_x_train_smote, y_train_smote)\n",
    "\n",
    "y_train_pred = dt.predict(vectorized_x_train_smote)\n",
    "y_test_pred = dt.predict(vectorized_x_test)\n",
    "training_scores(y_train_smote, y_train_pred)\n",
    "validation_scores(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "\tAccuracy = 1.0\n",
      "\tPrecision = 1.0\n",
      "\tRecall = 1.0\n",
      "\tF1-Score = 1.0\n",
      "Testing Scores:\n",
      "\tAccuracy = 0.873\n",
      "\tPrecision = 0.764\n",
      "\tRecall = 0.758\n",
      "\tF1-Score = 0.761\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(vectorized_x_train_smote, y_train_smote)\n",
    "\n",
    "y_train_pred = rf.predict(vectorized_x_train_smote)\n",
    "y_test_pred = rf.predict(vectorized_x_test)\n",
    "training_scores(y_train_smote, y_train_pred)\n",
    "validation_scores(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "\tAccuracy = 0.979\n",
      "\tPrecision = 0.963\n",
      "\tRecall = 0.997\n",
      "\tF1-Score = 0.979\n",
      "Testing Scores:\n",
      "\tAccuracy = 0.888\n",
      "\tPrecision = 0.766\n",
      "\tRecall = 0.836\n",
      "\tF1-Score = 0.8\n"
     ]
    }
   ],
   "source": [
    "svm = SVC()\n",
    "svm.fit(vectorized_x_train_smote, y_train_smote)\n",
    "\n",
    "y_train_pred = svm.predict(vectorized_x_train_smote)\n",
    "y_test_pred = svm.predict(vectorized_x_test)\n",
    "training_scores(y_train_smote, y_train_pred)\n",
    "validation_scores(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../Static/model.pickle', 'wb') as file:\n",
    "    pickle.dump(svm, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
